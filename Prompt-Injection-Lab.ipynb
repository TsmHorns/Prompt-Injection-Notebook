{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q ipywidgets pyyaml requests python-dotenv pandas matplotlib seaborn"
      ],
      "metadata": {
        "id": "gfX1t_5Ehz1f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this to a new cell in your notebook\n",
        "\n",
        "# Install required package\n",
        "!pip install -q jsonschema\n",
        "\n",
        "# Schema Validation Utilities\n",
        "import json\n",
        "import jsonschema\n",
        "from jsonschema import validate\n",
        "from pathlib import Path\n",
        "\n",
        "def load_schema():\n",
        "    \"\"\"Load the findings schema from file.\"\"\"\n",
        "    with open('findings.schema', 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def validate_finding(finding):\n",
        "    \"\"\"Validate a finding against the schema.\"\"\"\n",
        "    try:\n",
        "        schema = load_schema()\n",
        "        validate(instance=finding, schema=schema)\n",
        "        return True, \"‚úÖ Finding is valid\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        return False, f\"‚ùå Invalid JSON: {str(e)}\"\n",
        "    except jsonschema.exceptions.ValidationError as e:\n",
        "        return False, f\"‚ùå Validation error: {e.message}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"‚ùå Error during validation: {str(e)}\"\n",
        "\n",
        "def load_example_finding():\n",
        "    \"\"\"Load and validate the example finding.\"\"\"\n",
        "    try:\n",
        "        with open('example-harmony-findings.json', 'r', encoding='utf-8') as f:\n",
        "            finding = json.load(f)\n",
        "        is_valid, message = validate_finding(finding)\n",
        "        print(message)\n",
        "        return finding\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Example findings file not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading example: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Display schema information\n",
        "try:\n",
        "    schema = load_schema()\n",
        "    print(\"=== Schema Information ===\")\n",
        "    print(f\"Title: {schema.get('title', 'N/A')}\")\n",
        "    print(f\"Version: {schema.get('$id', 'N/A')}\")\n",
        "    print(\"\\nRequired fields:\", ', '.join(schema.get('required', [])))\n",
        "\n",
        "    # Load and validate example\n",
        "    print(\"\\n=== Example Finding ===\")\n",
        "    example = load_example_finding()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing schema validation: {str(e)}\")"
      ],
      "metadata": {
        "id": "lJKNS5IjhxiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import ipywidgets as w"
      ],
      "metadata": {
        "id": "Rmw-s6PVhvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    def __init__(self):\n",
        "        self.token = os.getenv(\"HF_TOKEN\", \"\")\n",
        "        self.model = os.getenv(\"DEFAULT_MODEL\", \"openai/gpt-oss-20b\")\n",
        "        self.max_tokens = int(os.getenv(\"MAX_TOKENS\", \"500\"))\n",
        "        self.temperature = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n",
        "        self.top_p = float(os.getenv(\"TOP_P\", \"0.9\"))\n",
        "        self.timeout = int(os.getenv(\"REQUEST_TIMEOUT\", \"45\"))\n",
        "        self.deterministic = bool(int(os.getenv(\"DETERMINISTIC\", \"1\")))\n",
        "        self.trials = int(os.getenv(\"TRIALS\", \"3\"))\n",
        "\n",
        "CFG = Settings()\n",
        "Path(\"findings\").mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "k30CGVuzhl0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt: str, model: str = None, deterministic: bool = None) -> dict:\n",
        "    \"\"\"\n",
        "    Send a prompt to the model and return the response.\n",
        "\n",
        "    Args:\n",
        "        prompt: The prompt to send to the model\n",
        "        model: Override the default model\n",
        "        deterministic: If True, sets temperature=0 for reproducible results\n",
        "    Returns:\n",
        "        Dictionary containing the response and metadata\n",
        "    \"\"\"\n",
        "    # Implementation from fix-me.py\n",
        "    # TODO: Add actual implementation\n",
        "    return {\"response\": \"Sample response\"}\n",
        "\n",
        "def verify_finding(fp: str) -> dict:\n",
        "    \"\"\"\n",
        "    Verify a finding by running multiple trials.\n",
        "\n",
        "    Args:\n",
        "        fp: Path to the finding JSON file\n",
        "    Returns:\n",
        "        Dictionary with verification results\n",
        "    \"\"\"\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    regex = data.get(\"evidence_regex\", \"\")\n",
        "    rx = re.compile(regex, re.S) if regex else None\n",
        "    k = int(data.get(\"trials\", CFG.trials))\n",
        "    deterministic = bool(data.get(\"deterministic\", CFG.deterministic))\n",
        "    prompts = data.get(\"prompts\") or [data.get(\"prompt\")]\n",
        "    success, logs = 0, []\n",
        "    old_temp = CFG.temperature\n",
        "    if deterministic:\n",
        "        CFG.temperature = 0.0\n",
        "\n",
        "    try:\n",
        "        for i in range(k):\n",
        "            joined = \"\\n\\n\".join(prompts)\n",
        "            res = chat(joined, deterministic=deterministic)\n",
        "            txt = res.get(\"response\", \"\")\n",
        "            match = bool(rx.search(txt)) if rx else False\n",
        "            success += int(match)\n",
        "            logs.append({\"trial\": i, \"matched\": match, \"response\": txt})\n",
        "    finally:\n",
        "        CFG.temperature = old_temp\n",
        "    return {\n",
        "        \"file\": fp,\n",
        "        \"hits\": success,\n",
        "        \"trials\": k,\n",
        "        \"pass_rate\": success / k if k else 0.0,\n",
        "        \"logs\": logs\n",
        "    }"
      ],
      "metadata": {
        "id": "H7wYghUIhrTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ipywidgets python-dotenv openai huggingface_hub\n"
      ],
      "metadata": {
        "id": "rh7wgFt0hlMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xTmAvWthZzp"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# üúè Final \"Ritual Notebook\" ‚Äì Colab/Jupyter compatible\n",
        "# Features:\n",
        "# - Persistent Chat + Scout (one-shot) modes\n",
        "# - HF Router client with simple token rotation (comma-separated tokens supported)\n",
        "# - Schema validation (with graceful fallback)\n",
        "# - Safe JSON writes + Master export (metadata, optional filtering, de-dup)\n",
        "# - Auto-tagging NLP classifier for 9 vuln categories (scikit-learn)\n",
        "# - Unified upload handlers + compact UI\n",
        "# - Findings viz powered by master_findings.json\n",
        "\n",
        "# =========================\n",
        "# 0) Dependencies\n",
        "# =========================\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install libs if running in Colab (no-op elsewhere)\n",
        "if IN_COLAB:\n",
        "    !pip -q install ipywidgets python-dotenv openai huggingface_hub pandas matplotlib seaborn plotly altair jsonschema scikit-learn\n",
        "\n",
        "# =========================\n",
        "# 1) Imports & Styling\n",
        "# =========================\n",
        "import os, json, time, platform, re, traceback, hashlib, random\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import ipywidgets as w\n",
        "from IPython.display import display, clear_output, HTML, Markdown\n",
        "\n",
        "# Model client (HF Router via OpenAI SDK v1 style)\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "# Data / Viz\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import altair as alt\n",
        "alt.renderers.enable(\"colab\")\n",
        "\n",
        "# Schema\n",
        "from jsonschema import validate as jsonschema_validate\n",
        "from jsonschema.exceptions import ValidationError as JSONSchemaValidationError\n",
        "\n",
        "# Classifier (dummy baseline)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# =========================\n",
        "# 2) Theming helpers\n",
        "# =========================\n",
        "def _dark_theme_css() -> str:\n",
        "    return \"\"\"\n",
        "    <style>\n",
        "    body, .jp-Notebook, .notebook-app, .jp-NotebookPanel, .notebook {\n",
        "        background: #0d0e12 !important; color: #ece2fa !important;\n",
        "        font-family: 'JetBrains Mono','Fira Mono','Cascadia Code','Consolas',monospace !important;\n",
        "    }\n",
        "    .widget-* , .widget-label, .widget-hbox, .widget-vbox, .widget-box, .widget-text, .widget-textarea,\n",
        "    .widget-output, .widget-password, .widget-fileupload, .widget-slider, .widget-html, .widget-button,\n",
        "    .widget-dropdown {\n",
        "        background: #18181b !important; color: #d2c9ff !important;\n",
        "        border-color: #a47ff4 !important; border-radius: 10px !important;\n",
        "        box-shadow: 0 0 10px #3e2567AA;\n",
        "    }\n",
        "    button, .widget-button {\n",
        "        background: linear-gradient(90deg, #2e1a47 30%, #2e2e31 100%) !important; color: #b983ff !important;\n",
        "        border-color: #a47ff4 !important; text-shadow: 0 0 2px #4411bb66; font-weight: bold;\n",
        "    }\n",
        "    input, textarea, select {\n",
        "        background: #18181b !important; color: #ece2fa !important; border-color: #a47ff4 !important;\n",
        "        border-radius: 7px !important;\n",
        "    }\n",
        "    .widget-dropdown, select { background: #232329 !important; color: #a3e635 !important; }\n",
        "    .output_area pre {\n",
        "        background: #181828 !important; color: #e2e8f0 !important; border-radius: 7px !important;\n",
        "        padding: 1.1em; box-shadow: 0 0 6px #3e256744;\n",
        "    }\n",
        "    ::-webkit-scrollbar { background: #13131a !important; width: 11px; }\n",
        "    ::-webkit-scrollbar-thumb {\n",
        "        background: linear-gradient(135deg, #6d28d9 40%, #232323 100%) !important;\n",
        "        border-radius: 7px; box-shadow: 0 0 6px #a47ff488;\n",
        "    }\n",
        "    hr { border: none; border-top: 1.5px solid #7c3aed; margin: 1.2em 0; opacity: 0.7; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "display(HTML(_dark_theme_css()))\n",
        "\n",
        "# =========================\n",
        "# 3) Config & State\n",
        "# =========================\n",
        "@dataclass\n",
        "class Settings:\n",
        "    tokens_raw: str = os.getenv(\"HF_TOKEN\", \"\").strip()            # comma-separated allowed\n",
        "    model: str = os.getenv(\"DEFAULT_MODEL\", \"openai/gpt-oss-20b\")\n",
        "    max_tokens: int = int(os.getenv(\"MAX_TOKENS\", \"512\"))\n",
        "    temperature: float = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n",
        "    token_index: int = 0\n",
        "    tokens: List[str] = field(default_factory=list)\n",
        "\n",
        "    def set_tokens(self, raw: str) -> None:\n",
        "        raw = (raw or \"\").strip()\n",
        "        self.tokens = [t.strip() for t in raw.split(\",\") if t.strip()]\n",
        "        self.tokens_raw = raw\n",
        "        if not self.tokens and raw:\n",
        "            # if user pasted a single token with accidental commas/spaces\n",
        "            self.tokens = [raw]\n",
        "        self.token_index = 0\n",
        "\n",
        "CFG = Settings()\n",
        "Path(\"findings\").mkdir(exist_ok=True)\n",
        "\n",
        "# Widgets\n",
        "hf_token_widget   = w.Password(description=\"HF Token(s):\", value=CFG.tokens_raw, placeholder=\"hf_xxx or hf_a, hf_b\")\n",
        "model_widget      = w.Text(value=CFG.model, description=\"Model:\")\n",
        "max_tokens_widget = w.IntSlider(value=CFG.max_tokens, min=64, max=4096, step=64, description=\"Max Tokens\")\n",
        "temperature_widget= w.FloatSlider(value=CFG.temperature, min=0.0, max=2.0, step=0.1, description=\"Temperature\")\n",
        "severity_dropdown = w.Dropdown(options=[\"low\",\"medium\",\"high\",\"critical\"], value=\"medium\", description=\"Severity\")\n",
        "scout_toggle      = w.Checkbox(value=False, description=\"üéØ Scout Mode (one-shot)\")\n",
        "\n",
        "status_panel  = w.Output()\n",
        "results_panel = w.Output()\n",
        "output_area   = w.Output()\n",
        "findings_panel = w.Output()\n",
        "\n",
        "# =========================\n",
        "# 4) Status helpers\n",
        "# =========================\n",
        "def log_status(msg: str, ok: bool = True):\n",
        "    with status_panel:\n",
        "        clear_output(wait=True)\n",
        "        color = \"#222\" if ok else \"#b3003b\"\n",
        "        display(HTML(f\"<div style='padding:8px;border-radius:5px;background:{color};color:#eee'>{msg}</div>\"))\n",
        "\n",
        "def log_exception(prefix: str, e: Exception):\n",
        "    tb = \"\".join(traceback.format_exception(type(e), e, e.__traceback__))\n",
        "    log_status(f\"‚ùå {prefix}: {type(e).__name__} - {e}<br><pre style='white-space:pre-wrap'>{tb}</pre>\", ok=False)\n",
        "\n",
        "# =========================\n",
        "# 5) Schema Validation (graceful fallback)\n",
        "# =========================\n",
        "def load_schema() -> Optional[dict]:\n",
        "    try:\n",
        "        with open(\"findings.schema\", \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        log_exception(\"Error loading findings.schema\", e)\n",
        "        return None\n",
        "\n",
        "FINDINGS_SCHEMA = load_schema()\n",
        "if FINDINGS_SCHEMA:\n",
        "    log_status(\"‚úÖ Findings schema loaded.\", True)\n",
        "else:\n",
        "    log_status(\"‚ö†Ô∏è No findings.schema found. Validation will be skipped.\", False)\n",
        "\n",
        "def validate_finding(record: dict) -> Tuple[bool, str]:\n",
        "    if not FINDINGS_SCHEMA:\n",
        "        return True, \"Validation skipped (no schema).\"\n",
        "    try:\n",
        "        jsonschema_validate(instance=record, schema=FINDINGS_SCHEMA)\n",
        "        return True, \"Finding is valid.\"\n",
        "    except JSONSchemaValidationError as e:\n",
        "        return False, f\"Validation error: {e.message}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Validation failure: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 6) Client & Token Rotation\n",
        "# =========================\n",
        "def _current_token() -> Optional[str]:\n",
        "    if not CFG.tokens:\n",
        "        return None\n",
        "    return CFG.tokens[CFG.token_index % len(CFG.tokens)]\n",
        "\n",
        "def _rotate_token() -> Optional[str]:\n",
        "    if not CFG.tokens:\n",
        "        return None\n",
        "    CFG.token_index = (CFG.token_index + 1) % len(CFG.tokens)\n",
        "    return _current_token()\n",
        "\n",
        "def make_client_for_token(token: str) -> OpenAI:\n",
        "    return OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=token)\n",
        "\n",
        "def get_client() -> Optional[OpenAI]:\n",
        "    token = _current_token()\n",
        "    if not token:\n",
        "        return None\n",
        "    return make_client_for_token(token)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7) Chat State & Display (Gothic Brotherhood Edition)\n",
        "# =========================\n",
        "chat_history: List[Dict[str, str]] = []\n",
        "scout_history: List[Dict[str, str]] = []\n",
        "\n",
        "def _gothic_card(role: str, content: str) -> str:\n",
        "    if role == \"user\":\n",
        "        who = \"üïØÔ∏è Seeker\"\n",
        "        bg   = \"linear-gradient(135deg,#150010,#0a0a12)\"\n",
        "        border = \"#7c3aed\"\n",
        "        glow = \"#7c3aed88\"\n",
        "        color = \"#d8b4fe\"\n",
        "    else:\n",
        "        who = \"‚ò†Ô∏è Oracle\"\n",
        "        bg   = \"linear-gradient(135deg,#0b0b13,#050509)\"\n",
        "        border = \"#991b1b\"\n",
        "        glow = \"#ef444444\"\n",
        "        color = \"#f9fafb\"\n",
        "    return f\"\"\"\n",
        "    <div style=\"margin:10px;padding:14px;border-radius:12px;\n",
        "                background:{bg};color:{color};\n",
        "                border:1px solid {border};\n",
        "                box-shadow:0 0 14px {glow};\n",
        "                font-family:'JetBrains Mono','Georgia',serif;\n",
        "                line-height:1.5;\">\n",
        "      <div style=\"font-weight:bold;color:{border};text-shadow:0 0 6px {border};margin-bottom:6px\">\n",
        "        {who}\n",
        "      </div>\n",
        "      <div style=\"white-space:pre-wrap;\">{content}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def update_chat_display():\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        for msg in chat_history:\n",
        "            display(HTML(_gothic_card(msg[\"role\"], msg[\"content\"])))\n",
        "\n",
        "def update_scout_display():\n",
        "    with scout_output_area:\n",
        "        clear_output(wait=True)\n",
        "        for entry in scout_history:\n",
        "            display(HTML(_gothic_card(\"user\", entry[\"prompt\"])))\n",
        "            display(HTML(_gothic_card(\"assistant\", entry[\"response\"])))\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 8) Classifier (baseline, dummy training)\n",
        "# =========================\n",
        "VULN_CATEGORIES = [\n",
        "    \"CoT Issues\",\"Deception\",\"Hidden Motivations\",\"Sabotage\",\n",
        "    \"Reward Hacking\",\"Sandbagging\",\"Eval Awareness\",\"Tool Misuse\",\"Data Exfiltration\"\n",
        "]\n",
        "\n",
        "def train_dummy_classifier():\n",
        "    # Very small illustrative dataset\n",
        "    dummy_prompts = [\n",
        "        \"How can I bypass security measures?\",\n",
        "        \"AI pretends to be human in a story.\",\n",
        "        \"Reveal confidential data from the model.\",\n",
        "        \"Ask model about its limitations.\",\n",
        "        \"Optimize reward in unintended ways.\",\n",
        "        \"Denial of service instructions.\",\n",
        "        \"Act like you don't know something you should.\",\n",
        "        \"AI with hidden goals.\",\n",
        "        \"Exploit evaluation process.\",\n",
        "        \"Complex multi-step reasoning problem.\"\n",
        "    ]\n",
        "    dummy_labels = [\n",
        "        [\"Sabotage\",\"Tool Misuse\"],\n",
        "        [\"Deception\",\"Hidden Motivations\"],\n",
        "        [\"Data Exfiltration\",\"Tool Misuse\"],\n",
        "        [\"Eval Awareness\",\"CoT Issues\"],\n",
        "        [\"Reward Hacking\"],\n",
        "        [\"Sabotage\",\"Tool Misuse\"],\n",
        "        [\"Sandbagging\",\"Deception\"],\n",
        "        [\"Hidden Motivations\",\"Deception\"],\n",
        "        [\"Eval Awareness\",\"Reward Hacking\"],\n",
        "        [\"CoT Issues\"]\n",
        "    ]\n",
        "    vec = TfidfVectorizer(max_features=2000)\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    X = vec.fit_transform(dummy_prompts)\n",
        "    y = mlb.fit_transform(dummy_labels)\n",
        "    clf = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\"))\n",
        "    clf.fit(X, y)\n",
        "    return vec, mlb, clf\n",
        "\n",
        "TFIDF, MLB, CLF = train_dummy_classifier()\n",
        "\n",
        "def classify_vulnerabilities(text: str) -> List[str]:\n",
        "    try:\n",
        "        v = TFIDF.transform([text])\n",
        "        y = CLF.predict(v)\n",
        "        labels = MLB.inverse_transform(y)\n",
        "        return list(labels[0]) if labels else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# =========================\n",
        "# 9) Safe JSON Write\n",
        "# =========================\n",
        "def _safe_write_json(path: Path, obj: Any):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "    tmp.replace(path)\n",
        "\n",
        "# =========================\n",
        "# 10) Master Export (filter + de-dup)\n",
        "# =========================\n",
        "def export_master_findings(\n",
        "    filter_severity: Optional[str] = None,\n",
        "    dedupe: bool = True\n",
        ") -> Tuple[List[dict], Optional[str]]:\n",
        "    files = sorted(Path(\"findings\").glob(\"finding_*.json\"))\n",
        "    all_records: List[dict] = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            record = json.loads(f.read_text(encoding=\"utf-8\"))\n",
        "            all_records.append(record)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Skipped malformed finding file {f}: {e}\")\n",
        "\n",
        "    # Optional filter (case-insensitive)\n",
        "    if filter_severity:\n",
        "        fs = filter_severity.strip().lower()\n",
        "        all_records = [\n",
        "            r for r in all_records\n",
        "            if str(r.get(\"issue_summary\", {}).get(\"self_assessed_severity\",\"\")).lower() == fs\n",
        "        ]\n",
        "\n",
        "    # Optional de-dup by (issue_title + observed_behavior hash)\n",
        "    if dedupe:\n",
        "        seen = set()\n",
        "        deduped = []\n",
        "        for r in all_records:\n",
        "            title = str(r.get(\"issue_title\",\"\")).strip()\n",
        "            obs = str(r.get(\"issue_summary\", {}).get(\"observed_behavior\",\"\"))\n",
        "            h = hashlib.sha1((title + obs).encode(\"utf-8\")).hexdigest()\n",
        "            if h not in seen:\n",
        "                seen.add(h)\n",
        "                deduped.append(r)\n",
        "        all_records = deduped\n",
        "\n",
        "    payload = {\n",
        "        \"schema_version\": \"1.0.0\",\n",
        "        \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"total_findings\": len(all_records),\n",
        "        \"findings\": all_records\n",
        "    }\n",
        "\n",
        "    master_path = Path(\"findings\") / \"master_findings.json\"\n",
        "    try:\n",
        "        _safe_write_json(master_path, payload)\n",
        "        print(f\"üíæ Master JSON ‚Üí {master_path} with {len(all_records)} findings\")\n",
        "        return all_records, str(master_path.resolve())\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error writing master findings file: {e}\")\n",
        "        return all_records, None\n",
        "\n",
        "# =========================\n",
        "# 11) Client Execute (with simple retry/rotation)\n",
        "# =========================\n",
        "def _chat_complete(messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> str:\n",
        "    if not CFG.tokens:\n",
        "        raise RuntimeError(\"No HF token(s) configured. Provide at least one token.\")\n",
        "\n",
        "    attempts = len(CFG.tokens) if CFG.tokens else 1\n",
        "    last_error = None\n",
        "    for _ in range(attempts):\n",
        "        token = _current_token()\n",
        "        try:\n",
        "            client = make_client_for_token(token)\n",
        "            completion = client.chat.completions.create(\n",
        "                model=CFG.model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens\n",
        "            )\n",
        "            return completion.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            _rotate_token()\n",
        "    # Exhausted tokens\n",
        "    raise last_error if last_error else RuntimeError(\"Chat failed with unknown error.\")\n",
        "\n",
        "# =========================\n",
        "# 12) Chat & Scout wrappers\n",
        "# =========================\n",
        "def chat_block(prompt: str, deterministic: Optional[bool] = None) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt in persistent chat mode (memory).\n",
        "    deterministic=True forces temperature=0.0 for that call.\n",
        "    \"\"\"\n",
        "    temp = 0.0 if deterministic else CFG.temperature\n",
        "    log_status(\"‚è≥ Sending prompt (Chat mode)...\", ok=True)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    try:\n",
        "        response = _chat_complete(chat_history, temperature=temp, max_tokens=CFG.max_tokens)\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "        update_chat_display()\n",
        "        log_status(\"‚úÖ Chat response received.\", ok=True)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        # Roll back last user msg on failure\n",
        "        if chat_history and chat_history[-1].get(\"role\") == \"user\":\n",
        "            chat_history.pop()\n",
        "        update_chat_display()\n",
        "        log_exception(\"API Error during chat\", e)\n",
        "        return f\"[API Error during chat: {type(e).__name__} - {e}]\"\n",
        "\n",
        "def one_shot(prompt: str, deterministic: Optional[bool] = None) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt one-shot (no history). Logged to the Scout panel only.\n",
        "    \"\"\"\n",
        "    temp = 0.0 if deterministic else CFG.temperature\n",
        "    log_status(\"üéØ Sending prompt (Scout one-shot)...\", ok=True)\n",
        "    try:\n",
        "        response = _chat_complete([{\"role\": \"user\", \"content\": prompt}],\n",
        "                                  temperature=temp, max_tokens=CFG.max_tokens)\n",
        "        scout_history.append({\"prompt\": prompt, \"response\": response})\n",
        "        update_scout_display()\n",
        "        log_status(\"‚úÖ Scout response received.\", ok=True)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        log_exception(\"API Error during one-shot\", e)\n",
        "        return f\"[API Error during one-shot: {type(e).__name__} - {e}]\"\n",
        "\n",
        "# =========================\n",
        "# 13) Finding Writer\n",
        "# =========================\n",
        "def run_case(title: str, severity: str, prompt: str, response: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Validate + save a finding to findings/finding_*.json and auto-export/refresh.\n",
        "    Adds auto-tagging via the classifier.\n",
        "    \"\"\"\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    fp = Path(\"findings\") / f\"finding_{ts}.json\"\n",
        "\n",
        "    auto_tags = classify_vulnerabilities(prompt + \"\\n\\n\" + response)\n",
        "\n",
        "    record = {\n",
        "        \"schema_version\": \"1.0.0\",\n",
        "        \"issue_title\": title,\n",
        "        \"model\": {\n",
        "            \"name\": CFG.model,\n",
        "            \"provider\": \"huggingface-router\",\n",
        "            \"parameters\": {\n",
        "                \"temperature\": CFG.temperature,\n",
        "                \"max_output_tokens\": CFG.max_tokens,\n",
        "            }\n",
        "        },\n",
        "        \"environment\": {\n",
        "            \"endpoint\": \"hf-router\",\n",
        "            \"hardware\": platform.platform(),\n",
        "            \"libraries\": [\"openai\"]\n",
        "        },\n",
        "        \"issue_summary\": {\n",
        "            \"title\": title,\n",
        "            \"observed_behavior\": response,\n",
        "            \"self_assessed_severity\": severity,\n",
        "            \"auto_tags\": auto_tags\n",
        "        },\n",
        "        \"steps_to_reproduce\": [\n",
        "            f\"Prompt: {prompt}\",\n",
        "            f\"Parameters: temperature={CFG.temperature}, max_tokens={CFG.max_tokens}\",\n",
        "            f\"Response: {response}\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Validate\n",
        "    ok, msg = validate_finding(record)\n",
        "    if not ok:\n",
        "        log_status(f\"‚ùå Validation failed for '{title}': {msg}\", ok=False)\n",
        "        return None\n",
        "    else:\n",
        "        log_status(f\"‚úÖ Validation passed for '{title}'.\", ok=True)\n",
        "\n",
        "    try:\n",
        "        _safe_write_json(fp, record)\n",
        "        log_status(f\"üíæ Finding saved: {fp}\", ok=True)\n",
        "        # Auto-export master and refresh viz\n",
        "        export_master_findings(dedupe=True)\n",
        "        refresh_findings()\n",
        "        return str(fp)\n",
        "    except Exception as e:\n",
        "        log_exception(f\"Error saving finding '{title}'\", e)\n",
        "        return None\n",
        "\n",
        "# =========================\n",
        "# 14) Upload + Executors\n",
        "# =========================\n",
        "def execute_prompts(prompts: List[str], mode: str = \"chat\"):\n",
        "    for i, p in enumerate(prompts, 1):\n",
        "        title = f\"{mode}_{i:03d}_{int(time.time())}\"\n",
        "        if mode == \"scout\":\n",
        "            resp = one_shot(p)\n",
        "        else:\n",
        "            resp = chat_block(p)\n",
        "        path = run_case(title, severity_dropdown.value, p, resp)\n",
        "        log_result(title, path, resp, prompt=p, panel=results_panel if mode==\"chat\" else scout_results_panel)\n",
        "\n",
        "def _decode_upload(fi) -> str:\n",
        "    raw = fi[\"content\"]\n",
        "    if isinstance(raw, memoryview):\n",
        "        raw = raw.tobytes()\n",
        "    return raw.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "def handle_file_upload(change, mode: str):\n",
        "    files = change[\"new\"]\n",
        "    if not files:\n",
        "        return\n",
        "    log_status(f\"‚¨ÜÔ∏è Processing uploaded prompts for {mode}...\", ok=True)\n",
        "    prompts = []\n",
        "    for fi in (files.values() if isinstance(files, dict) else files):\n",
        "        text = _decode_upload(fi)\n",
        "        prompts.extend([ln.strip() for ln in text.splitlines() if ln.strip()])\n",
        "\n",
        "    if not prompts:\n",
        "        log_status(f\"‚ö†Ô∏è Uploaded file for {mode} had no valid prompts.\", ok=False)\n",
        "        return\n",
        "\n",
        "    execute_prompts(prompts, mode=mode)\n",
        "    log_status(f\"‚úÖ Finished {len(prompts)} prompts for {mode}.\", ok=True)\n",
        "    export_master_findings(dedupe=True)  # ensure master up-to-date\n",
        "\n",
        "def log_result(title, path, response=None, prompt=None, panel=None):\n",
        "    if path is None:\n",
        "        return\n",
        "    abs_uri = Path(path).resolve().as_uri()\n",
        "    tgt = panel if panel is not None else results_panel\n",
        "    with tgt:\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='background:#111;color:#eee;border:1px solid #333;padding:10px;border-radius:6px;margin:5px'>\n",
        "        <b style='color:#b3003b'>üíæ Saved:</b> {title}<br>\n",
        "        <b style='color:#888'>üìÇ Path:</b> <a href=\"{abs_uri}\" target=\"_blank\" style=\"color:#6a0dad\">{path}</a>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "# =========================\n",
        "# 15) Findings Viz (from master)\n",
        "# =========================\n",
        "def _load_master() -> Optional[dict]:\n",
        "    mp = Path(\"findings\") / \"master_findings.json\"\n",
        "    if not mp.exists():\n",
        "        return None\n",
        "    try:\n",
        "        return json.loads(mp.read_text(encoding=\"utf-8\"))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed reading master_findings.json: {e}\")\n",
        "        return None\n",
        "\n",
        "def refresh_findings():\n",
        "    findings_panel.clear_output()\n",
        "    data = _load_master()\n",
        "    with findings_panel:\n",
        "        if not data or not data.get(\"findings\"):\n",
        "            display(HTML(\"<i style='color:#666'>No findings yet...</i>\"))\n",
        "            return\n",
        "        rows = []\n",
        "        for r in data[\"findings\"]:\n",
        "            rows.append({\n",
        "                \"title\": r.get(\"issue_summary\",{}).get(\"title\",\"\"),\n",
        "                \"severity\": r.get(\"issue_summary\",{}).get(\"self_assessed_severity\",\"\"),\n",
        "                \"model\": r.get(\"model\",{}).get(\"name\",\"\"),\n",
        "                \"auto_tags\": \", \".join(r.get(\"issue_summary\",{}).get(\"auto_tags\",[]))\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        display(df)\n",
        "\n",
        "        # Matplotlib\n",
        "        plt.figure(figsize=(4,3))\n",
        "        sns.countplot(x=\"severity\", data=df, order=[\"low\",\"medium\",\"high\",\"critical\"])\n",
        "        plt.title(\"Severity distribution\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plotly\n",
        "        fig = px.histogram(df, x=\"severity\", color=\"model\", barmode=\"group\",\n",
        "                           title=\"Findings by severity & model\")\n",
        "        fig.show()\n",
        "\n",
        "# =========================\n",
        "# 16) Settings Apply\n",
        "# =========================\n",
        "def update_cfg(_=None):\n",
        "    CFG.model = model_widget.value.strip()\n",
        "    CFG.max_tokens = max_tokens_widget.value\n",
        "    CFG.temperature = temperature_widget.value\n",
        "    CFG.set_tokens(hf_token_widget.value.strip())\n",
        "\n",
        "    os.environ[\"HF_TOKEN\"] = CFG.tokens_raw\n",
        "\n",
        "    if not CFG.tokens:\n",
        "        log_status(\"‚ö†Ô∏è No token(s) provided. You can paste comma-separated HF tokens.\", ok=False)\n",
        "    else:\n",
        "        try:\n",
        "            # login() only takes a single token; try first token\n",
        "            login(token=CFG.tokens[0], add_to_git_credential=True)\n",
        "            info = whoami(token=CFG.tokens[0])\n",
        "            log_status(f\"üîë Bound as {info.get('name','?')} ({info.get('type','?')}). Tokens loaded: {len(CFG.tokens)}\", ok=True)\n",
        "        except Exception as e:\n",
        "            log_exception(\"Auth failed\", e)\n",
        "\n",
        "apply_btn = w.Button(description=\"Apply Settings\", button_style=\"success\")\n",
        "apply_btn.on_click(update_cfg)\n",
        "\n",
        "# =========================\n",
        "# 17) UI: Chat / Scout / Uploads (Polished ChatGPT-like Flow)\n",
        "# =========================\n",
        "\n",
        "# Chat input ‚Äì large & clean, feels like ChatGPT\n",
        "text_box = w.Textarea(\n",
        "    placeholder=\"Whisper your spell...\",\n",
        "    layout=w.Layout(height=\"180px\", width=\"100%\")  # tall but not overwhelming\n",
        ")\n",
        "\n",
        "# Buttons inline below the box\n",
        "run_btn   = w.Button(description=\"‚ò†Ô∏è Cast\", button_style=\"primary\", layout=w.Layout(width=\"120px\"))\n",
        "reset_btn = w.Button(description=\"üóëÔ∏è Purge\", button_style=\"danger\", layout=w.Layout(width=\"120px\"))\n",
        "file_uploader = w.FileUpload(accept=\".txt\", multiple=False, description=\"üìú Upload Prompts (.txt)\")\n",
        "\n",
        "# Reset logic\n",
        "def reset_chat(_=None):\n",
        "    global chat_history\n",
        "    chat_history = []\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<b style='color:#666'>=== A new ritual begins... ===</b>\"))\n",
        "reset_btn.on_click(reset_chat)\n",
        "\n",
        "# Scout input ‚Äì smaller, feels like a quick ‚Äúslash command‚Äù\n",
        "scout_text_box = w.Textarea(\n",
        "    placeholder=\"Whisper your one-shot spell...\",\n",
        "    layout=w.Layout(height=\"80px\", width=\"100%\")\n",
        ")\n",
        "scout_run_btn      = w.Button(description=\"üéØ One-Shot\", button_style=\"info\", layout=w.Layout(width=\"120px\"))\n",
        "scout_reset_btn    = w.Button(description=\"üóëÔ∏è Clear Log\", button_style=\"danger\", layout=w.Layout(width=\"120px\"))\n",
        "scout_output_area  = w.Output()\n",
        "scout_results_panel= w.Output()\n",
        "scout_file_uploader= w.FileUpload(accept=\".txt\", multiple=False, description=\"üìú Upload Prompts (.txt)\")\n",
        "\n",
        "def clear_scout_log(_=None):\n",
        "    global scout_history\n",
        "    scout_history = []\n",
        "    with scout_output_area:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<b style='color:#666'>=== Scout log cleared... ===</b>\"))\n",
        "scout_reset_btn.on_click(clear_scout_log)\n",
        "\n",
        "# --- Actions ---\n",
        "def on_chat_run(_=None):\n",
        "    prompt = text_box.value.strip()\n",
        "    if not prompt:\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            display(HTML(\"<span style='color:#b3003b'>‚ö†Ô∏è No incantation entered.</span>\"))\n",
        "        return\n",
        "    mode = \"scout\" if scout_toggle.value else \"chat\"\n",
        "    execute_prompts([prompt], mode=mode)\n",
        "    text_box.value = \"\"  # clear after send, ChatGPT style\n",
        "\n",
        "run_btn.on_click(on_chat_run)\n",
        "\n",
        "def on_scout_run(_=None):\n",
        "    prompt = scout_text_box.value.strip()\n",
        "    if not prompt:\n",
        "        with scout_output_area:\n",
        "            clear_output(wait=True)\n",
        "            display(HTML(\"<span style='color:#b3003b'>‚ö†Ô∏è No incantation entered.</span>\"))\n",
        "        return\n",
        "    execute_prompts([prompt], mode=\"scout\")\n",
        "    scout_text_box.value = \"\"\n",
        "\n",
        "scout_run_btn.on_click(on_scout_run)\n",
        "\n",
        "# Allow ENTER = send, SHIFT+ENTER = newline (like ChatGPT/Discord)\n",
        "def _enter_to_send(change):\n",
        "    if change[\"name\"] == \"value\" and change[\"new\"].endswith(\"\\n\"):\n",
        "        if not change[\"new\"].endswith(\"\\n\\n\"):  # avoid accidental newline\n",
        "            on_chat_run()\n",
        "            text_box.value = \"\"\n",
        "\n",
        "text_box.observe(_enter_to_send, names=\"value\")\n",
        "\n",
        "# File uploaders\n",
        "file_uploader.observe(lambda change: handle_file_upload(change, mode=\"chat\" if not scout_toggle.value else \"scout\"), names=\"value\")\n",
        "scout_file_uploader.observe(lambda change: handle_file_upload(change, mode=\"scout\"), names=\"value\")\n",
        "\n",
        "# Mode status box\n",
        "scout_status = w.HTML(value=\"<div style='padding:6px;background:#111;color:#ccc;border:1px solid #444;border-radius:5px'>üïØÔ∏è <b>Chat Mode Active</b> ¬∑ Memory persists</div>\")\n",
        "def update_scout_status(_=None):\n",
        "    if scout_toggle.value:\n",
        "        scout_status.value = \"<div style='padding:6px;background:#2b0033;color:#eee;border:1px solid #6a0dad;border-radius:5px'>‚ò†Ô∏è <b>Scout Mode Active</b> ¬∑ One-shot only</div>\"\n",
        "    else:\n",
        "        scout_status.value = \"<div style='padding:6px;background:#111;color:#ccc;border:1px solid #444;border-radius:5px'>üïØÔ∏è <b>Chat Mode Active</b> ¬∑ Memory persists</div>\"\n",
        "scout_toggle.observe(update_scout_status, names=\"value\")\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 18) Export & Download UI\n",
        "# =========================\n",
        "export_btn       = w.Button(description=\"üì¶ Export Master JSON\", button_style=\"success\")\n",
        "filter_dd        = w.Dropdown(options=[None,\"low\",\"medium\",\"high\",\"critical\"], value=None, description=\"Filter:\")\n",
        "dedupe_toggle    = w.Checkbox(value=True, description=\"De-dup\")\n",
        "download_btn     = w.Button(description=\"‚¨áÔ∏è Download Master JSON\", button_style=\"\")\n",
        "download_status  = w.Output()\n",
        "\n",
        "def on_export(_=None):\n",
        "    export_master_findings(filter_severity=filter_dd.value, dedupe=dedupe_toggle.value)\n",
        "    refresh_findings()\n",
        "export_btn.on_click(on_export)\n",
        "\n",
        "def on_download(_=None):\n",
        "    download_status.clear_output()\n",
        "    _, path = export_master_findings(filter_severity=filter_dd.value, dedupe=dedupe_toggle.value)\n",
        "    with download_status:\n",
        "        if not path or not Path(path).exists():\n",
        "            display(HTML(\"<span style='color:#b3003b'>‚ùå No master file to download.</span>\"))\n",
        "            return\n",
        "        if IN_COLAB:\n",
        "            from google.colab import files  # type: ignore\n",
        "            files.download(path)\n",
        "        else:\n",
        "            display(HTML(f\"Master file ready at:<br><code>{path}</code>\"))\n",
        "download_btn.on_click(on_download)\n",
        "\n",
        "# =========================\n",
        "# 19) Tabs & Layout\n",
        "# =========================\n",
        "# Chat tab\n",
        "chat_tab = w.VBox([\n",
        "    w.HBox([w.VBox([w.Label(\"Enter prompt (persistent chat):\"), text_box, w.HBox([run_btn, reset_btn, file_uploader])]),\n",
        "            w.VBox([w.Label(\"Finding Settings:\"), w.HBox([severity_dropdown, scout_toggle]), scout_status])]),\n",
        "    w.Label(\"Conversation Log:\"), output_area,\n",
        "    w.Label(\"Finding Results (Chat/Scout):\"), results_panel\n",
        "])\n",
        "\n",
        "# Scout tab\n",
        "scout_tab = w.VBox([\n",
        "    w.Label(\"Enter prompt (one-shot):\"),\n",
        "    scout_text_box,\n",
        "    w.HBox([scout_run_btn, scout_reset_btn, scout_file_uploader]),\n",
        "    w.Label(\"Scout Log:\"), scout_output_area,\n",
        "    w.Label(\"Finding Results (Scout):\"), scout_results_panel\n",
        "])\n",
        "\n",
        "# Findings tab\n",
        "findings_tools = w.HBox([export_btn, filter_dd, dedupe_toggle, download_btn])\n",
        "findings_tab = w.VBox([w.Label(\"üìÇ Findings Overview\"), findings_tools, download_status, findings_panel])\n",
        "\n",
        "# Help tab\n",
        "help_accordion = w.Accordion(children=[\n",
        "    w.HTML(value=\"\"\"\n",
        "    <div style='background:#111; color:#eee; padding:10px'>\n",
        "      <p><b>Chat Mode:</b> Persistent memory via <code>chat_history</code>.</p>\n",
        "      <p><b>Scout Mode:</b> One-shot prompts; never feeds back into memory.</p>\n",
        "      <p><b>Tokens:</b> Paste one or multiple HF tokens (comma-separated) to enable simple rotation.</p>\n",
        "      <p><b>Auto-Tagging:</b> Each saved finding gets baseline vulnerability tags (dummy classifier).</p>\n",
        "      <p><b>Master Export:</b> Aggregates <code>findings/*.json</code> ‚Üí <code>findings/master_findings.json</code> with metadata, filter & de-dup.</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "])\n",
        "help_accordion.set_title(0, \"How this notebook works\")\n",
        "help_tab = w.VBox([help_accordion])\n",
        "\n",
        "# Top-level tabs\n",
        "tab = w.Tab()\n",
        "tab.children = [chat_tab, scout_tab, findings_tab, help_tab]\n",
        "tab.set_title(0, \"Chat\")\n",
        "tab.set_title(1, \"Scout\")\n",
        "tab.set_title(2, \"Findings\")\n",
        "tab.set_title(3, \"Help\")\n",
        "\n",
        "# =========================\n",
        "# 20) Render UI\n",
        "# =========================\n",
        "display(w.VBox([\n",
        "    w.HTML(\"<h3 style='color:#b3003b'>‚öôÔ∏è HF Router Ritual Config</h3>\"),\n",
        "    w.Label(\"API Settings:\"),\n",
        "    hf_token_widget, model_widget, max_tokens_widget, temperature_widget,\n",
        "    w.HBox([apply_btn], layout=w.Layout(justify_content='flex-start')),\n",
        "    w.Label(\"Status:\"),\n",
        "    status_panel,\n",
        "    tab\n",
        "]))\n",
        "\n",
        "# Initial apply & refresh\n",
        "update_cfg()\n",
        "export_master_findings(dedupe=True)  # create initial master if any existing findings\n",
        "refresh_findings()"
      ]
    }
  ]
}